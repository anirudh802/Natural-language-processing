{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 16:18:02.863184: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-23 16:18:03.958941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/runcandel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/runcandel/anaconda3/envs/yolo7/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "stpwrds=stopwords.words(\"english\")\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoModelForTableQuestionAnswering,pipeline, AutoTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>para</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75041</th>\n",
       "      <td>2008_Summer_Olympics_torch_relay</td>\n",
       "      <td>In Japan, the Mayor of Nagano, Shoichi Washiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75046</th>\n",
       "      <td>2008_Summer_Olympics_torch_relay</td>\n",
       "      <td>The Olympic Flame is supposed to remain lit fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75048</th>\n",
       "      <td>2008_Summer_Olympics_torch_relay</td>\n",
       "      <td>A union planned to protest at the relay for be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75049</th>\n",
       "      <td>2008_Summer_Olympics_torch_relay</td>\n",
       "      <td>Chinese media have also reported on Jin Jing, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75051</th>\n",
       "      <td>2008_Summer_Olympics_torch_relay</td>\n",
       "      <td>Two additional teams of 40 attendants each wil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  topic  \\\n",
       "75041  2008_Summer_Olympics_torch_relay   \n",
       "75046  2008_Summer_Olympics_torch_relay   \n",
       "75048  2008_Summer_Olympics_torch_relay   \n",
       "75049  2008_Summer_Olympics_torch_relay   \n",
       "75051  2008_Summer_Olympics_torch_relay   \n",
       "\n",
       "                                                    para  \n",
       "75041  In Japan, the Mayor of Nagano, Shoichi Washiza...  \n",
       "75046  The Olympic Flame is supposed to remain lit fo...  \n",
       "75048  A union planned to protest at the relay for be...  \n",
       "75049  Chinese media have also reported on Jin Jing, ...  \n",
       "75051  Two additional teams of 40 attendants each wil...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel(r\"/home/runcandel/Desktop/pclubsectask/nlp/paragraphs.xlsx\")\n",
    "df=df.drop_duplicates(subset=['para'])\n",
    "df=df.drop(columns=['idig'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>para</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15557</td>\n",
       "      <td>15556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>362</td>\n",
       "      <td>15556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>New_York_City</td>\n",
       "      <td>Zhejiang (help·info), formerly romanized as C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                topic                                               para\n",
       "count           15557                                              15556\n",
       "unique            362                                              15556\n",
       "top     New_York_City   Zhejiang (help·info), formerly romanized as C...\n",
       "freq              146                                                  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a function to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(st):\n",
    "    st=re.sub(r'[^\\w\\s]',\"\",st)\n",
    "    stemm=PorterStemmer()\n",
    "    st=st.split()\n",
    "    st=[x for x in st if not x in stpwrds]\n",
    "    st=[stemm.stem(x) for x in st]\n",
    "    return \" \".join(st)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating function check similarity bw texts by checking number of common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simcheck(a,b):\n",
    "    q=0\n",
    "    for i in a:\n",
    "        if i in b:\n",
    "            q+=1\n",
    "    return q \n",
    "\n",
    "\n",
    "def mostsim(query,seq):\n",
    "    l=[0,0,0,0,0]\n",
    "    k=[0,0,0,0,0]\n",
    "    for i in range(len(seq)):\n",
    "        a=simcheck(query,seq[i])\n",
    "        if a>min(k):\n",
    "            for j in range(5):\n",
    "                if k[j]<a:\n",
    "                    l.insert(j,i)\n",
    "                    k.insert(j,a)\n",
    "                    l.pop(5)\n",
    "                    k.pop(5)\n",
    "                    break\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### created a list named data with all the paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df['para'].astype(\"str\").to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### created a tokenizer and created dataf with processed data and seq with the tokenized sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3365, 205, 41260, 290, 1845, 271, 3365, 959, 15254, 10467, 101, 24706, 1911, 16746, 118, 8242, 103, 129, 129, 271, 372, 1145, 4115, 30378, 239, 547], [1, 217, 30379, 327, 41261, 41262, 41263, 323, 217, 30380, 327, 2522, 785, 7030, 622, 3931, 151, 7030, 1298, 19, 4453, 37173, 2008, 11644, 327, 2008, 30381, 158, 4630, 151, 1320, 30382, 237, 82, 327, 217, 30381, 7031, 41265, 11923, 262, 30383, 217, 224, 9223, 357, 41266, 16747, 16748], [3365, 807, 41267, 30384, 6, 419, 990, 211, 995, 807, 218, 327, 450, 105, 8544, 30385, 30384, 1761, 18622], [1, 121, 156, 3365, 110, 4056, 14075, 455, 126, 3078, 341, 536, 121, 7887, 74, 9223, 1, 273, 9223, 159, 1371, 392, 550, 1616, 3681, 123, 17684, 273, 9223, 274, 3365, 41271, 1573, 706, 12851, 14075, 2495, 15255, 169, 2240, 273, 9223, 6609, 3645, 273, 101, 209, 271, 622, 1645, 392, 1616, 3681, 123, 13145, 1, 869, 9223, 41273, 41274, 1753, 41275, 41276, 5780, 869, 820, 9223, 392, 2584, 101, 271, 3366, 271, 41277, 21152, 146, 16749, 341, 1, 869, 450, 9223, 2593, 4035, 1933, 1090, 101, 3366, 271, 1, 9223, 3250, 550, 3041, 1, 6517, 30386, 3661, 5781, 3041, 41278, 41279, 7031, 41280, 28450, 9223, 41282, 66, 9223, 4, 235, 5426, 435, 273, 4631, 16750, 8150, 9223, 75, 278, 30386, 9223, 16267, 282, 2903, 30389, 341, 1, 9223, 278, 1071, 269, 1067, 197, 146, 2255, 16747, 323, 4631, 21156, 13, 292, 113, 944, 9223, 16750, 11647, 341, 8439, 3202, 9223, 323, 282, 16267, 13145, 12334, 341, 5357, 217, 16747, 3365, 4631, 15254], [16747, 113, 916, 15257, 18624, 15257, 15255, 3218, 13145, 8439, 273, 16750, 1489, 1656, 75, 93, 1656, 188, 121, 763, 1316, 2255, 16747, 2584, 13146, 358, 9223, 19639, 3631, 221], [21, 1871, 98, 273, 490, 2698, 3365, 197, 16112, 24708, 41286, 6860, 18625, 767, 1350, 1002, 2698, 1002, 41287, 1831, 273, 4631, 247, 16747, 1307, 228, 2270, 4470, 5215, 10611, 6861, 274, 271, 30561, 1831, 425, 1602, 509, 5427, 41289, 103, 2411], [3365, 97, 4631, 98, 273, 4631, 74, 290, 4631, 1002, 4631, 1307, 66, 148, 98, 273, 2698, 1, 1894, 98, 273, 392, 3365, 668, 1, 6879, 66, 41291, 41292, 10992, 41293, 1112, 2076, 847, 11685, 4631, 4, 3517, 924, 4631, 52, 66, 14196, 18626, 4631, 8380, 6989], [74, 30393, 1316, 30380, 323, 217, 7030, 995, 35, 98, 110, 962, 269, 103, 110, 1299, 269, 101, 271, 1, 52, 269, 103, 30393, 7969, 4, 24710, 30394, 7030], [1724, 1003, 4631, 6861, 4631, 24711, 8854, 2720, 10530, 101, 45, 60, 587, 221, 308, 5215, 121, 103, 327, 45, 5215, 101, 271, 953, 121, 294, 7030, 11, 4256, 24712, 18624, 6862, 66, 10992, 1656, 209, 2873, 101, 271, 98, 273, 123, 8140, 8246, 7030, 151, 743, 24712, 41295, 41296, 4091, 217, 24712, 1, 4933, 8284, 154, 599, 328, 74, 143, 19, 2240, 238, 99, 113, 5215, 209, 2873, 953, 735, 5215, 103, 327, 1336, 30977, 1112, 10021, 282, 3365, 5215, 953, 121, 103, 271, 1145, 3365, 10021, 282, 962, 15804, 101, 15254, 118, 8242, 16746, 14077, 289, 5357, 228, 6312, 246, 962, 121, 3682, 2496, 5215, 412, 66, 10992, 198, 174, 339, 209, 2873, 101, 271, 45, 5215, 101, 271, 953, 174, 103, 271, 4820, 221, 113, 101, 1, 21160, 8854, 66, 292, 209, 2873, 103, 327], [3365, 18629, 21161, 327, 5326, 995, 7032, 121, 501, 10022, 13149, 7033, 46, 13149, 2271, 82, 13149, 2497, 101, 103, 1059, 2411, 1, 983, 144, 13149, 2497, 631, 1120, 2840, 271, 1, 2840, 1784, 3882, 3365, 962, 269, 7030, 13146, 218, 30379, 327, 4748, 7030, 1298, 21156, 101, 271, 2873, 1, 41298, 1506, 68, 144, 13149, 8856, 7033, 2107, 304, 3365, 6957, 269, 21161, 129, 2875, 18631, 318, 7033, 1, 1120, 2840, 995, 639, 1784, 3882, 101, 271, 2873, 962, 269, 7, 7033, 3365, 228, 273]]\n",
      "413\n"
     ]
    }
   ],
   "source": [
    "tekken= Tokenizer()\n",
    "\n",
    "tekken.fit_on_texts(data)\n",
    "dataf=[]\n",
    "for i in range(len(data)):\n",
    "    dataf.append(pp(data[i]))\n",
    "\n",
    "seq=tekken.texts_to_sequences(dataf)\n",
    "print(seq[:10])\n",
    "print(tekken.word_index['japanese'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[547, 13269, 1665]]\n",
      "[15552, 374, 15528, 0, 14]\n",
      "In Japan, the Mayor of Nagano, Shoichi Washizawa said that it has become a \"great nuisance\" for the city to host the torch relay prior to the Nagano leg. Washizawa's aides said the mayor's remark was not criticism about the relay itself but about the potential disruptions and confusion surrounding it.　A city employee of the Nagano City Office ridiculed the protests in Europe, he said \"They are doing something foolish\", in a televised interview. Nagano City officially apologized later and explained what he had wanted to say was \"Such violent protests were not easy to accept\". Also citing concerns about protests as well as the recent violence in Tibet, a major Buddhist temple in Nagano cancelled its plans to host the opening stage of the Olympic torch relay, this temple was vandalised by an un-identified person the day after in apparent revenge,\n",
      "Finally, in the 1990s, Internet Protocol-based videoconferencing became possible, and more efficient video compression technologies were developed, permitting desktop, or personal computer (PC)-based videoconferencing. In 1992 CU-SeeMe was developed at Cornell by Tim Dorcey et al. In 1995 the first public videoconference between North America and Africa took place, linking a technofair in San Francisco with a techno-rave and cyberdeli in Cape Town. At the Winter Olympics opening ceremony in Nagano, Japan, Seiji Ozawa conducted the Ode to Joy from Beethoven's Ninth Symphony simultaneously across five continents in near-real time.\n",
      "\n",
      " Japan: The event was held in Nagano, which hosted the 1998 Winter Olympics, on April 26. Japanese Buddhist temple Zenkō-ji, which was originally scheduled to be the starting point for the Olympic torch relay in Nagano, refused to host the torch and pulled out of the relay plans, amid speculation that monks there sympathized with anti-Chinese government protesters. as well as the risk of disruption by violent protests. Parts of Zenkō-ji temple's main building (Zenkō-ji Hondō), reconstructed in 1707 and one of the National Treasures of Japan, was then vandalized with spraypaint. A new starting point, previously the site of a municipal building and now a parking lot, was chosen by the city. An event the city had planned to hold at the Minami Nagano Sports Park following the torch relay was also canceled out of concern about disruptions caused by demonstrators protesting against China's recent crackdown in Tibet. Thousands of riot police were mobilized to protect the torch along its route. The show of force kept most protesters in check, but slogans shouted by pro-China or pro-Tibet demonstrators, Japanese nationalists, and human rights organizations flooded the air. Five men were arrested and four injured amidst scenes of mob violence. The torch route was packed with mostly peaceful demonstrators. The public was not allowed at the parking lot where the relay started. After the Zenkoji monks held a prayer ceremony for victims of the recent events in Tibet. More than 100 police officers ran with the torch and riot police lined the streets while three helicopters flew above. Only two Chinese guards were allowed to accompany the torch because of Japan's concern over their treatment of demonstrators at previous relays. A man with a Tibetan flag tried to stop the torch at the beginning of the relay but was dragged off by police. Some raw eggs were also thrown from the crowd.\n",
      " Zhejiang (help·info), formerly romanized as Chekiang, is an eastern coastal province of China. Zhejiang is bordered by Jiangsu province and Shanghai municipality to the north, Anhui province to the northwest, Jiangxi province to the west, and Fujian province to the south; to the east is the East China Sea, beyond which lie the Ryukyu Islands of Japan.\n",
      "During the Second Sino-Japanese War, which led into World War II, much of Zhejiang was occupied by Japan and placed under the control of the Japanese puppet state known as the Reorganized National Government of China. Following the Doolittle Raid, most of the B-25 American crews that came down in China eventually made it to safety with the help of Chinese civilians and soldiers. The Chinese people who helped them, however, paid dearly for sheltering the Americans. The Imperial Japanese Army began the Zhejiang-Jiangxi Campaign to intimidate the Chinese out of helping downed American airmen. The Japanese killed an estimated 250,000 civilians while searching for Doolittle’s men.\n"
     ]
    }
   ],
   "source": [
    "ques=\"japan nagano mayor\"\n",
    "lis=[ques]\n",
    "qt=tekken.texts_to_sequences(lis)\n",
    "print(qt)\n",
    "print(mostsim(qt[0],seq))\n",
    "l=mostsim(qt[0],seq)\n",
    "for i in mostsim(qt[0],seq):\n",
    "    print(data[i])\n",
    "\n",
    "def freq(ques,seq):\n",
    "    lis=[ques]\n",
    "    qt=tekken.texts_to_sequences(lis)\n",
    "    # print(qt)\n",
    "    # print(mostsim(qt[0],seq))\n",
    "    l=mostsim(qt[0],seq)\n",
    "    for i in mostsim(qt[0],seq):\n",
    "        print(data[i])\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the same above approach but this time using tf-idf vectors and cosine similarity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0477435 0.        0.        ... 0.        0.        1.       ]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "ques=\"japan nagano mayor\"\n",
    "dataf.append(pp(ques))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(dataf)\n",
    "cssn=cosine_similarity(tfidf_matrix)\n",
    "dataf.pop(-1)\n",
    "\n",
    "# feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(cssn[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15552, 5225, 4566, 13608, 14843]\n"
     ]
    }
   ],
   "source": [
    "l2=[0,0,0,0,0]\n",
    "k=[0,0,0,0,0]\n",
    "for i in range(len(cssn[-1])-1):\n",
    "    if cssn[-1,i]>min(k):\n",
    "        for j in range(5):\n",
    "            if k[j]<cssn[-1,i]:\n",
    "                l2.insert(j,i)\n",
    "                k.insert(j,cssn[-1,i])\n",
    "                l2.pop(5)\n",
    "                k.pop(5)\n",
    "                break\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Japan, the Mayor of Nagano, Shoichi Washizawa said that it has become a \"great nuisance\" for the city to host the torch relay prior to the Nagano leg. Washizawa's aides said the mayor's remark was not criticism about the relay itself but about the potential disruptions and confusion surrounding it.　A city employee of the Nagano City Office ridiculed the protests in Europe, he said \"They are doing something foolish\", in a televised interview. Nagano City officially apologized later and explained what he had wanted to say was \"Such violent protests were not easy to accept\". Also citing concerns about protests as well as the recent violence in Tibet, a major Buddhist temple in Nagano cancelled its plans to host the opening stage of the Olympic torch relay, this temple was vandalised by an un-identified person the day after in apparent revenge,\n",
      "Patience Latting was elected Mayor of Oklahoma City in 1971, becoming the city's first female mayor. Latting was also the first woman to serve as mayor of a U.S. city with over 350,000 residents.\n",
      "Plymouth was granted the dignity of Lord Mayor by King George V in 1935. The position is elected each year by a group of six councillors. It is traditional that the position of the Lord Mayor alternates between the Conservative Party and the Labour Party annually and that the Lord Mayor chooses the Deputy Lord Mayor. Conservative councillor Dr John Mahony is the incumbent for 2015–16.\n",
      "Boston has a strong mayor – council government system in which the mayor (elected every fourth year) has extensive executive power. Marty Walsh became Mayor in January 2014, his predecessor Thomas Menino's twenty-year tenure having been the longest in the city's history. The Boston City Council is elected every two years; there are nine district seats, and four citywide \"at-large\" seats. The School Committee, which oversees the Boston Public Schools, is appointed by the mayor.\n",
      "Ann Arbor has a council-manager form of government. The City Council has 11 voting members: the mayor and 10 city council members. The mayor and city council members serve two-year terms: the mayor is elected every even-numbered year, while half of the city council members are up for election annually (five in even-numbered and five in odd-numbered years). Two council members are elected from each of the city's five wards. The mayor is elected citywide. The mayor is the presiding officer of the City Council and has the power to appoint all Council committee members as well as board and commission members, with the approval of the City Council. The current mayor of Ann Arbor is Christopher Taylor, a Democrat who was elected as mayor in 2014. Day-to-day city operations are managed by a city administrator chosen by the city council.\n"
     ]
    }
   ],
   "source": [
    "for i in l2:\n",
    "    print(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### making a function for the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getting the question answer model to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Shoichi Washiz\n"
     ]
    }
   ],
   "source": [
    "def answer(l,ques,model,tokenizer):\n",
    "    for i in range(len(l)):\n",
    "        context = data[l[i]]\n",
    "        inputs = tokenizer(ques, context, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        start_position = outputs.start_logits[0].argmax()\n",
    "        end_position = outputs.end_logits[0].argmax()\n",
    "        answer_ids = inputs[\"input_ids\"][0][start_position:end_position]\n",
    "        return tokenizer.decode(answer_ids)\n",
    "ques='who was japan nagano mayor'\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(answer(l,ques,model,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[10807, 14422, 10815, 10811, 14406]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/runcandel/nltk_data'\n    - '/home/runcandel/anaconda3/envs/yolo7/nltk_data'\n    - '/home/runcandel/anaconda3/envs/yolo7/share/nltk_data'\n    - '/home/runcandel/anaconda3/envs/yolo7/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m r\u001b[38;5;241m=\u001b[39mRake()\n\u001b[0;32m----> 2\u001b[0m r\u001b[38;5;241m.\u001b[39mextract_keywords_from_text(data[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(r\u001b[38;5;241m.\u001b[39mget_phrases_with_scores())\n",
      "File \u001b[0;32m~/anaconda3/envs/yolo7/lib/python3.12/site-packages/rake_nltk/rake.py:126\u001b[0m, in \u001b[0;36mRake.extract_keywords_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_keywords_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Method to extract keywords from the text provided.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    :param text: Text to extract keywords from, provided as a string.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     sentences: List[Sentence] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize_text_to_sentences(text)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_keywords_from_sentences(sentences)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolo7/lib/python3.12/site-packages/rake_nltk/rake.py:180\u001b[0m, in \u001b[0;36mRake._tokenize_text_to_sentences\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tokenize_text_to_sentences\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Sentence]:\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Tokenizes the given text string into sentences using the configured\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    sentence tokenizer. Configuration uses `nltk.tokenize.sent_tokenize`\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    by default.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    :return: List of sentences as per the tokenizer used.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_tokenizer(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolo7/lib/python3.12/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolo7/lib/python3.12/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/envs/yolo7/lib/python3.12/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/anaconda3/envs/yolo7/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/runcandel/nltk_data'\n    - '/home/runcandel/anaconda3/envs/yolo7/nltk_data'\n    - '/home/runcandel/anaconda3/envs/yolo7/share/nltk_data'\n    - '/home/runcandel/anaconda3/envs/yolo7/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "r=Rake()\n",
    "r.extract_keywords_from_text(data[0])\n",
    "print(r.get_phrases_with_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "mod=spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simspa(ques,para):\n",
    "    a=mod(ques)\n",
    "    l=[0,0,0,0,0]\n",
    "    k=[0,0,0,0,0]\n",
    "    for i in range(len(para)):\n",
    "        b=mod(para[i])\n",
    "        h=a.similarity(b)\n",
    "        if h>min(k):\n",
    "            for j in range(5):\n",
    "                if k[j]<h:\n",
    "                    l.insert(j,i)\n",
    "                    k.insert(j,h)\n",
    "                    l.pop(5)\n",
    "                    k.pop(5)\n",
    "                    break\n",
    "    for i in l:\n",
    "        print(data[i])\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winners receive the FA Cup trophy, of which there have been two designs and five actual cups; the latest is a 2014 replica of the second design, introduced in 1911. Winners also qualify for European football and a place in the FA Community Shield match. Arsenal are the current holders, having beaten Aston Villa 4–0 in the 2015 final to win the cup for the second year in a row. It was their 12th FA Cup title overall, making Arsenal the FA Cup's most successful club ahead of Manchester United on 11.\n",
      "Arsenal Ladies are the women's football club affiliated to Arsenal. Founded in 1987, they turned semi-professional in 2002 and are managed by Clare Wheatley. Arsenal Ladies are the most successful team in English women's football. In the 2008–09 season, they won all three major English trophies – the FA Women's Premier League, FA Women's Cup and FA Women's Premier League Cup, and, as of 2009, were the only English side to have won the UEFA Women's Cup, having done so in the 2006–07 season as part of a unique quadruple. The men's and women's clubs are formally separate entities but have quite close ties; Arsenal Ladies are entitled to play once a season at the Emirates Stadium, though they usually play their home matches at Boreham Wood.\n",
      "The final is normally held the Saturday after the Premier League season finishes in May. The only seasons in recent times when this pattern was not followed were 1999–2000, when most rounds were played a few weeks earlier than normal as an experiment, and 2010–11 and 2012–13 when the FA Cup Final was played before the Premier League season had finished, to allow Wembley Stadium to be ready for the UEFA Champions League final, as well as in 2011–12 to allow England time to prepare for that summer's European Championships.\n",
      "It is very rare for top clubs to miss the competition, although it can happen in exceptional circumstances. Defending holders Manchester United did not enter the 1999–2000 FA Cup, as they were already in the inaugural Club World Championship, with the club stating that entering both tournaments would overload their fixture schedule and make it more difficult to defend their Champions League and Premiership titles. The club claimed that they did not want to devalue the FA Cup by fielding a weaker side. The move benefited United as they received a two-week break and won the 1999–2000 league title by an 18-point margin, although they did not progress past the group stage of the Club World Championship. The withdrawal from the FA Cup, however, drew considerable criticism as this weakened the tournament's prestige and Sir Alex Ferguson later admitted his regret regarding their handling of the situation.\n",
      "For many years Arsenal's away colours were white shirts and either black or white shorts. In the 1969–70 season, Arsenal introduced an away kit of yellow shirts with blue shorts. This kit was worn in the 1971 FA Cup Final as Arsenal beat Liverpool to secure the double for the first time in their history. Arsenal reached the FA Cup final again the following year wearing the red and white home strip and were beaten by Leeds United. Arsenal then competed in three consecutive FA Cup finals between 1978 and 1980 wearing their \"lucky\" yellow and blue strip, which remained the club's away strip until the release of a green and navy away kit in 1982–83. The following season, Arsenal returned to the yellow and blue scheme, albeit with a darker shade of blue than before.\n",
      "[10807, 14422, 10815, 10811, 14406]\n"
     ]
    }
   ],
   "source": [
    "print(simspa(\"How many times was the FA cup playing in the Oval ?\",data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
